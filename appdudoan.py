import streamlit as st
import cv2
import numpy as np
import joblib
from PIL import Image
from skimage.feature import hog
import os

# =======================
# LOAD MODEL + SCALER
# =======================
emotion_model = joblib.load("emotion_model.pkl")
model = emotion_model["model"]

# N·∫øu scaler l∆∞u ·ªü ƒë∆∞·ªùng d·∫´n ri√™ng
scaler_path = emotion_model.get("scaler_path", None)
if scaler_path and os.path.exists(scaler_path):
    scaler = joblib.load(scaler_path)
else:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y scaler! Ki·ªÉm tra l·∫°i file .pkl")
    st.stop()

labels = emotion_model["labels"]

# =======================
# LOAD FACE DETECTOR (DNN)
# =======================
CONFIG_FILE = r"E:\DudoanCamxuc\deploy.prototxt"
MODEL_FILE = r"E:\DudoanCamxuc\res10_300x300_ssd_iter_140000.caffemodel"

if not os.path.exists(CONFIG_FILE):
    st.error(f"Kh√¥ng t√¨m th·∫•y file c·∫•u h√¨nh: {CONFIG_FILE}")
    st.stop()
if not os.path.exists(MODEL_FILE):
    st.error(f"Kh√¥ng t√¨m th·∫•y file model: {MODEL_FILE}")
    st.stop()

net = cv2.dnn.readNetFromCaffe(CONFIG_FILE, MODEL_FILE)

# =======================
# STREAMLIT UI
# =======================
st.title("üòÑ ·ª®ng d·ª•ng nh·∫≠n di·ªán c·∫£m x√∫c khu√¥n m·∫∑t (HOG + DNN)")
st.write("T·∫£i ·∫£nh l√™n, h·ªá th·ªëng s·∫Ω ph√°t hi·ªán khu√¥n m·∫∑t v√† d·ª± ƒëo√°n c·∫£m x√∫c t·ª´ng ng∆∞·ªùi.")

uploaded_file = st.file_uploader("üì∏ Ch·ªçn ·∫£nh", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    img = Image.open(uploaded_file)
    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)

    # === (1) Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ ph√°t hi·ªán m·∫∑t t·ªët h∆°n ===
    img_cv = cv2.convertScaleAbs(img_cv, alpha=1.2, beta=15)  # tƒÉng t∆∞∆°ng ph·∫£n, s√°ng h∆°n
    img_cv = cv2.GaussianBlur(img_cv, (3, 3), 0)              # gi·∫£m nhi·ªÖu nh·∫π

    (h, w) = img_cv.shape[:2]
    blob = cv2.dnn.blobFromImage(cv2.resize(img_cv, (300, 300)), 1.0,
                                 (300, 300), (104.0, 177.0, 123.0))
    net.setInput(blob)
    detections = net.forward()

    count = 0
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]
        # === (2) Gi·∫£m ng∆∞·ª°ng confidence ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c m·∫∑t nghi√™ng / b·ªã che ===
        if confidence < 0.15:
            continue

        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
        (x1, y1, x2, y2) = box.astype("int")
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w - 1, x2), min(h - 1, y2)

        # ====== C·∫ÆT KHU√îN M·∫∂T ======
        face = img_cv[y1:y2, x1:x2]
        if face.size == 0:
            continue

        # === (3) L√†m m·ªãn v√πng khu√¥n m·∫∑t ƒë·ªÉ HOG ·ªïn ƒë·ªãnh ===
        face = cv2.resize(face, (96, 96))
        face = cv2.GaussianBlur(face, (3, 3), 0)

        gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
        face_resized = cv2.resize(gray, (48, 48), interpolation=cv2.INTER_AREA)

        # ====== TR√çCH XU·∫§T HOG ======
        features, _ = hog(
            face_resized,
            orientations=9,
            pixels_per_cell=(8, 8),
            cells_per_block=(2, 2),
            block_norm='L2-Hys',
            visualize=True
        )
        features = features.reshape(1, -1)

        if features.shape[1] != scaler.n_features_in_:
            st.warning(f"S·ªë ƒë·∫∑c tr∆∞ng HOG kh√¥ng kh·ªõp: {features.shape[1]} vs {scaler.n_features_in_}")
            continue

        features_scaled = scaler.transform(features)
        pred = model.predict(features_scaled)[0]

        if isinstance(labels, list):
            try:
                label = labels[int(pred)]
            except:
                label = str(pred)
        else:
            label = str(pred)

        cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img_cv, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)

        count += 1

    if count == 0:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t n√†o r√µ r√†ng trong ·∫£nh.")
    else:
        st.image(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB),
                 caption="K·∫øt qu·∫£ nh·∫≠n di·ªán c·∫£m x√∫c",
                 use_container_width=True)
